[2024-09-23 15:40:46,146][root][INFO] - {'prompts': 'data/prompts/math/prompts.json', 'start_prompts': 0, 'end_prompts': 2500, 'n_return': 16, 'save_file_scores': 'data/train/scores_2.5_llama_tag.json', 'save_file_responses': 'data/train/responses_2.5_llama_tag.json', 'save_train_responses': 'data/train/train_2.5_llama_tag.json', 'save_train_labels': 'data/train/labels_2.5_llama_tag.json', 'model_config': {'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'download_dir': '/scr/jphilipp/stargate/pretrained_models/Meta-Llama-3.1-8B-Instruct', 'dtype': 'auto', 'tensor_parallel_size': 1}, 'generation_config': {'max_new_tokens': 2048, 'num_return_sequences': 1, 'best_of': 1, 'temperature': 1.0}}
WARNING 09-23 15:40:46 arg_utils.py:776] The model has a long context length (131072). This may cause OOM errors during the initial memory profiling phase, or result in low performance due to small KV cache space. Consider setting --max-model-len to a smaller value.
INFO 09-23 15:40:46 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='meta-llama/Meta-Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/scr/jphilipp/stargate/pretrained_models/Meta-Llama-3.1-8B-Instruct', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=meta-llama/Meta-Llama-3.1-8B-Instruct, use_v2_block_manager=False, enable_prefix_caching=False)
INFO 09-23 15:40:47 model_runner.py:720] Starting to load model meta-llama/Meta-Llama-3.1-8B-Instruct...
INFO 09-23 15:40:47 weight_utils.py:225] Using model weights format ['*.safetensors']
INFO 09-23 15:40:50 model_runner.py:732] Loading model weights took 14.9888 GB
INFO 09-23 15:40:58 gpu_executor.py:102] # GPU blocks: 20540, # CPU blocks: 2048
INFO 09-23 15:41:00 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 09-23 15:41:00 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 09-23 15:41:09 model_runner.py:1225] Graph capturing finished in 9 secs.
NAME
    generate.py

SYNOPSIS
    generate.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     json
       JSON (JavaScript Object Notation) <http://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     re
       Support for regular expressions (RE).

     fire
       The Python Fire module.

     random
       Random variable generators.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

COMMANDS
    COMMAND is one of the following:

     DictConfig
       Container tagging interface

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     load_dataset
       Load a dataset from the Hugging Face Hub, or a local dataset.

     VLLMInferenceModel
       Simple vLLM Inference Wrapper for text generation and logprobs.

     clean_numbers

     last_boxed_only
       Given a (q,a) sample, filter the answers so that they only contain the last oxed{...} or box{...} element

     last_boxed_only_string

     remove_boxed

     is_equiv

     main

VALUES
    VALUE is one of the following:

     PROMPT

     SYSTEM_PROMPT

     USER_PROMPT

     ASSISTANT_PROMPT
