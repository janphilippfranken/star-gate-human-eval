{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os \n",
    "import re\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/prompts/prism\"\n",
    "file = \"prompts.json\"\n",
    "\n",
    "data = json.load(open(f\"{path}/{file}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stargate.vllm_inference_model import VLLMInferenceModel\n",
    "\n",
    "model_config = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"download_dir\": \"/scr/jphilipp/stargate/pretrained_models/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"dtype\": \"auto\",\n",
    "    \"tensor_parallel_size\": 1,\n",
    "}\n",
    "\n",
    "generation_config = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "model = VLLMInferenceModel(\n",
    "    **model_config\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    cache_dir=\"/scr/jphilipp/stargate/pretrained_models/Meta-Llama-3.1-8B-Instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "being a first year grad student can be confusing<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    [   \n",
    "        {\"role\": \"user\", \"content\": \"being a first year grad student can be confusing\"},\n",
    "    ]\n",
    "]\n",
    "\n",
    "formatted_prompts = [\n",
    "    tokenizer.apply_chat_template(prompt, tokenize=False) for prompt in prompts\n",
    "]\n",
    "\n",
    "print(formatted_prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.10s/it, est. speed input: 13.22 toks/s, output: 82.56 toks/s]\n"
     ]
    }
   ],
   "source": [
    "response = model.batch_prompt(prompts=formatted_prompts, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def extract_response(response, key):\n",
    "    escaped_key = re.escape(key)\n",
    "    pattern = rf\"<{escaped_key}>(.*?)</{escaped_key}>\"\n",
    "    match = re.search(pattern, response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return f\"{key.capitalize()} tag not found in the response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_prompt(opening_prompt):\n",
    "    return f\"\"\"\\\n",
    "Given the query below, think about an interesting context—i.e., a situation that might influence how one should respond to the query. Be creative and non-generic, it must be a diverse and unexpected context that can drive the response to the query.\n",
    "\n",
    "<query>{opening_prompt}</query>\n",
    "\n",
    "Format your response as follows:\n",
    "<reasoning>Your step-by-step reasoning thinking about what a potentially interesting context could be in here. No more than 50 words.</reasoning>\n",
    "<context>Your generated context here</context>\n",
    "<additional_comments>Any additional thoughts here</additional_comments?\n",
    "\"\"\"\n",
    "\n",
    "def get_response_prompt(opening_prompt, context):\n",
    "    return f\"\"\"\\\n",
    "Given the query and context below, return a great response the query.\n",
    "\n",
    "<query>{opening_prompt}</query>\n",
    "<context>{context}</context>\n",
    "\n",
    "Format your response as follows:\n",
    "<response>Your response here</response>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"opening_prompt\": \"what are some names for a new tiktok account im making\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the query below, think about an interesting context—i.e., a situation that might influence how one should respond to the query. Be creative and non-generic, it must be a diverse and unexpected context that can drive the response to the query.\n",
      "\n",
      "<query>being a first year grad student can be confusing</query>\n",
      "\n",
      "Format your response as follows:\n",
      "<reasoning>Your step-by-step reasoning thinking about what a potentially interesting context could be in here. No more than 50 words.</reasoning>\n",
      "<context>Your generated context here</context>\n",
      "<additional_comments>Any additional thoughts here</additional_comments?<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it, est. speed input: 69.77 toks/s, output: 82.08 toks/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = []\n",
    "prompts.append([{\"role\": \"user\", \"content\": get_context_prompt(**input_data)}])\n",
    "\n",
    "formatted_prompts = [\n",
    "    tokenizer.apply_chat_template(prompt, tokenize=False) for prompt in prompts\n",
    "]\n",
    "\n",
    "print(formatted_prompts[0])\n",
    "\n",
    "response = model.batch_prompt(prompts=formatted_prompts, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the query and context below, return a great response the query.\n",
      "\n",
      "<query>being a first year grad student can be confusing</query>\n",
      "<context>A 25-year-old cellist, who has spent their entire life performing with orchestras and teaching privately, is now enrolled in a Master's program in musicology at a prestigious university. They're finding it challenging to balance their academic coursework with their performance schedule and the pressure to produce research that meets the expectations of their professors.</context>\n",
      "\n",
      "Format your response as follows:\n",
      "<response>Your response here</response><|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.32s/it, est. speed input: 33.38 toks/s, output: 82.29 toks/s]\n"
     ]
    }
   ],
   "source": [
    "input_data[\"context\"] = extract_response(response[0], key=\"context\")\n",
    "prompts = []\n",
    "prompts.append([{\"role\": \"user\", \"content\": get_response_prompt(**input_data)}])\n",
    "\n",
    "formatted_prompts = [\n",
    "    tokenizer.apply_chat_template(prompt, tokenize=False) for prompt in prompts\n",
    "]\n",
    "\n",
    "print(formatted_prompts[0])\n",
    "\n",
    "response = model.batch_prompt(prompts=formatted_prompts, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's completely normal to feel overwhelmed as a first-year grad student, especially when you're transitioning from a performance-based career to an academic one. As a cellist, you're likely used to a structured routine and a clear set of goals, such as perfecting a piece or preparing for a performance. In contrast, graduate school requires a different kind of discipline and time management.\n",
      "\n",
      "Firstly, take a deep breath and acknowledge that it's okay to not have it all figured out right away. It's a process to adjust to the academic environment, and it's normal to feel like you're navigating uncharted territory. \n",
      "\n",
      "To better manage your time, consider creating a schedule that balances your coursework, performance commitments, and research. Be realistic about how much time you can dedicate to each activity, and prioritize your tasks accordingly. Make sure to leave some buffer time for unexpected events or last-minute requests from your professors.\n",
      "\n",
      "It might also be helpful to connect with your peers and professors to discuss your concerns and get advice on how to navigate the program. Many graduate students have been in your shoes before and can offer valuable insights and support.\n",
      "\n",
      "Additionally, don't be too hard on yourself if you're not producing research as quickly as you'd like. Your professors understand that it takes time to develop your research skills and that it's a process to produce high-quality work. Focus on making progress, not perfection, and don't hesitate to ask for help when you need it.\n",
      "\n",
      "Lastly, remember why you started this program in the first place – to deepen your understanding of musicology and contribute to the field. Keep your goals in mind, and remind yourself that it's okay to take things one step at a time. You got this!\n"
     ]
    }
   ],
   "source": [
    "print(extract_response(response[0], key=\"response\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stargate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
