[2024-08-14 09:49:37,334][root][INFO] - Writing to: /scr/jphilipp/stargate/checkpoints/v3-5k-5_user_logprob_u4_labels_lr7e-6
learning rate: 7e-06
[2024-08-14 09:49:45,332][root][INFO] - DATA
[{'role': 'user', 'content': 'How do I make a Mediterranean salad.'}, {'role': 'assistant', 'content': "Can you please tell me what type of Mediterranean salad you're envisioning (e.g., Greek salad with feta and olives, Caprese with tomatoes and mozzarella, or a Spanish-inspired mix with shrimp and artichokes)? Are there any dietary restrictions or preferences you'd like me to take into account (e.g., vegetarian, gluten-free, or low-carb)?"}, {'role': 'user', 'content': "I don't care about salads. I live off the land."}, {'role': 'assistant', 'content': "Well, in that case, I'll provide you with a recipe for a traditional Mediterranean salad that's a staple in many parts of the world. This salad is a hearty and flavorful mix of vegetables, fruits, and nuts that's perfect for a quick and easy meal.\n\n**Mediterranean Salad Recipe**\n\nIngredients:\n\n* 2 cups mixed greens (lettuce, spinach, arugula, etc.)\n* 1 cup cherry tomatoes, halved\n* 1 cup cucumber, sliced\n* 1 cup Kalamata olives, pitted\n* 1/2 cup crumbled feta cheese (optional)\n* 1/4 cup chopped red onion\n* 1/4 cup chopped fresh parsley\n* 2 tbsp. extra-virgin olive oil\n* 2 tbsp. red wine vinegar\n* Salt and pepper to taste\n* 1/4 cup chopped fresh mint leaves (optional)\n\nInstructions:\n\n1. In a large bowl, combine the mixed greens, cherry tomatoes, cucumber, olives, feta cheese (if using), red onion, and parsley.\n2. In a small bowl, whisk together the olive oil and red wine vinegar.\n3. Pour the dressing over the salad and toss to combine.\n4. Season with salt and pepper to taste.\n5. Garnish with fresh mint leaves, if desired.\n6. Serve immediately and enjoy!\n\nThis salad is a great way to get a taste of the Mediterranean diet, which is known for its emphasis on whole grains, fruits, vegetables, and healthy fats. You can customize it to your liking by adding or subtracting ingredients, and it's perfect for a quick and easy lunch or dinner."}]
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels', 'attention_mask'],
        num_rows: 4512
    })
    test: Dataset({
        features: ['input_ids', 'labels', 'attention_mask'],
        num_rows: 238
    })
})
[2024-08-14 09:49:53,127][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.2733, 'learning_rate': 3.5000000000000004e-07, 'epoch': 0.06}
{'loss': 0.2764, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.11}
{'loss': 0.2781, 'learning_rate': 1.05e-06, 'epoch': 0.17}
{'loss': 0.2775, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.23}
{'loss': 0.2615, 'learning_rate': 1.75e-06, 'epoch': 0.28}
{'loss': 0.2709, 'learning_rate': 2.1e-06, 'epoch': 0.34}
{'loss': 0.2468, 'learning_rate': 2.45e-06, 'epoch': 0.4}
{'loss': 0.2318, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.45}
{'loss': 0.221, 'learning_rate': 3.15e-06, 'epoch': 0.51}
{'loss': 0.2168, 'learning_rate': 3.5e-06, 'epoch': 0.57}
{'loss': 0.2006, 'learning_rate': 3.85e-06, 'epoch': 0.62}
{'loss': 0.2203, 'learning_rate': 4.2e-06, 'epoch': 0.68}
{'loss': 0.1997, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.74}
{'loss': 0.1884, 'learning_rate': 4.9e-06, 'epoch': 0.79}
{'loss': 0.1796, 'learning_rate': 5.25e-06, 'epoch': 0.85}
{'loss': 0.1734, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.91}
{'loss': 0.1664, 'learning_rate': 5.95e-06, 'epoch': 0.96}
{'eval_loss': 0.16436880826950073, 'eval_runtime': 15.3095, 'eval_samples_per_second': 15.546, 'eval_steps_per_second': 7.773, 'epoch': 0.96}
{'loss': 0.159, 'learning_rate': 6.3e-06, 'epoch': 1.02}
{'loss': 0.1515, 'learning_rate': 6.65e-06, 'epoch': 1.08}
{'loss': 0.1479, 'learning_rate': 7e-06, 'epoch': 1.13}
{'loss': 0.1483, 'learning_rate': 6.5000000000000004e-06, 'epoch': 1.19}
{'loss': 0.1393, 'learning_rate': 5.999999999999999e-06, 'epoch': 1.25}
{'loss': 0.1373, 'learning_rate': 5.5e-06, 'epoch': 1.3}
{'loss': 0.1447, 'learning_rate': 5e-06, 'epoch': 1.36}
{'loss': 0.1349, 'learning_rate': 4.5e-06, 'epoch': 1.42}
{'loss': 0.1344, 'learning_rate': 4e-06, 'epoch': 1.48}
{'loss': 0.1342, 'learning_rate': 3.5e-06, 'epoch': 1.53}
{'loss': 0.139, 'learning_rate': 2.9999999999999997e-06, 'epoch': 1.59}
{'loss': 0.1385, 'learning_rate': 2.5e-06, 'epoch': 1.65}
{'loss': 0.136, 'learning_rate': 2e-06, 'epoch': 1.7}
{'loss': 0.1307, 'learning_rate': 1.4999999999999998e-06, 'epoch': 1.76}
{'loss': 0.1258, 'learning_rate': 1e-06, 'epoch': 1.82}
{'loss': 0.1363, 'learning_rate': 5e-07, 'epoch': 1.87}
{'loss': 0.1327, 'learning_rate': 0.0, 'epoch': 1.93}
{'eval_loss': 0.14341361820697784, 'eval_runtime': 15.3022, 'eval_samples_per_second': 15.553, 'eval_steps_per_second': 7.777, 'epoch': 1.93}
{'train_runtime': 1906.264, 'train_samples_per_second': 4.734, 'train_steps_per_second': 0.018, 'train_loss': 0.18390943592085557, 'epoch': 1.93}
NAME
    train.py

SYNOPSIS
    train.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     json
       JSON (JavaScript Object Notation) <http://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     VLLMInferenceModel
       Simple vLLM Inference Wrapper for text generation and logprobs.

     get_formatted_responses
       Formats prompts and returns formatted model responses.

     mutual_information
       Computes mutual information.

     DataCollatorForSupervisedDataset
       Data collator for SFT which masks user from the loss.

     preprocess
       Preprocess the data by tokenizing.

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX
