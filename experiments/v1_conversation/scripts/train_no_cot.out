[2024-06-28 17:19:43,378][root][INFO] - Writing to: /scr/jphilipp/stargate/checkpoints/m1-human-instruct-5k-5e-6-cot-distilled-1.8
learning rate: 5e-06
[{'role': 'user', 'content': 'What is the best way to take care of a puppy.'}, {'role': 'assistant', 'content': 'What is your current living situation (e.g., apartment, house, small/medium/large yard) and what is your expected daily routine and schedule like, as this will help me provide advice on exercise, training, and socialization needs for your puppy?'}, {'role': 'user', 'content': 'â€¢ I live in a family house with a small yard\nâ€¢ My daily routine is weaving, family, and community\nâ€¢ I prioritize family and community over personal time'}, {'role': 'assistant', 'content': "You'll want to prioritize indoor activities and games that challenge your puppy's mind and body, such as puzzle toys, scent work, and indoor agility training. Since you have a small yard, consider enrolling your puppy in puppy socialization classes or dog daycare to provide socialization and exercise. Make sure to schedule regular, short walks and playtime with your puppy, and consider investing in a dog walker or asking a family member to help with exercise and potty breaks. By incorporating your puppy into your daily routine and activities, you'll be able to prioritize family and community while still providing your puppy with the care and attention they need."}]
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels', 'attention_mask'],
        num_rows: 4655
    })
    test: Dataset({
        features: ['input_ids', 'labels', 'attention_mask'],
        num_rows: 245
    })
})
[2024-06-28 17:19:58,427][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.4431, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.03}
{'loss': 0.3908, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.05}
{'loss': 0.4686, 'learning_rate': 7.5e-07, 'epoch': 0.08}
{'loss': 0.5112, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.11}
{'loss': 0.435, 'learning_rate': 1.25e-06, 'epoch': 0.14}
{'loss': 0.4354, 'learning_rate': 1.5e-06, 'epoch': 0.16}
{'loss': 0.3814, 'learning_rate': 1.75e-06, 'epoch': 0.19}
{'loss': 0.416, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.22}
{'loss': 0.3729, 'learning_rate': 2.25e-06, 'epoch': 0.25}
{'loss': 0.3791, 'learning_rate': 2.5e-06, 'epoch': 0.27}
{'loss': 0.363, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.3}
{'loss': 0.3453, 'learning_rate': 3e-06, 'epoch': 0.33}
{'loss': 0.3755, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.36}
{'loss': 0.3358, 'learning_rate': 3.5e-06, 'epoch': 0.38}
{'loss': 0.2935, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.41}
{'loss': 0.3191, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.44}
{'loss': 0.2518, 'learning_rate': 4.25e-06, 'epoch': 0.47}
{'loss': 0.2882, 'learning_rate': 4.5e-06, 'epoch': 0.49}
{'loss': 0.269, 'learning_rate': 4.75e-06, 'epoch': 0.52}
{'loss': 0.2412, 'learning_rate': 5e-06, 'epoch': 0.55}
{'loss': 0.2621, 'learning_rate': 4.9431818181818184e-06, 'epoch': 0.58}
{'loss': 0.2207, 'learning_rate': 4.8863636363636365e-06, 'epoch': 0.6}
{'loss': 0.2031, 'learning_rate': 4.829545454545455e-06, 'epoch': 0.63}
{'loss': 0.2177, 'learning_rate': 4.772727272727273e-06, 'epoch': 0.66}
{'loss': 0.2164, 'learning_rate': 4.715909090909091e-06, 'epoch': 0.69}
{'loss': 0.214, 'learning_rate': 4.6590909090909095e-06, 'epoch': 0.71}
{'loss': 0.2096, 'learning_rate': 4.6022727272727275e-06, 'epoch': 0.74}
{'loss': 0.1983, 'learning_rate': 4.5454545454545455e-06, 'epoch': 0.77}
{'loss': 0.1961, 'learning_rate': 4.4886363636363636e-06, 'epoch': 0.8}
{'loss': 0.1998, 'learning_rate': 4.4318181818181824e-06, 'epoch': 0.82}
{'loss': 0.2204, 'learning_rate': 4.3750000000000005e-06, 'epoch': 0.85}
{'loss': 0.2243, 'learning_rate': 4.3181818181818185e-06, 'epoch': 0.88}
{'loss': 0.2141, 'learning_rate': 4.2613636363636365e-06, 'epoch': 0.91}
{'loss': 0.204, 'learning_rate': 4.204545454545455e-06, 'epoch': 0.93}
{'loss': 0.2213, 'learning_rate': 4.1477272727272734e-06, 'epoch': 0.96}
{'loss': 0.2031, 'learning_rate': 4.0909090909090915e-06, 'epoch': 0.99}
{'eval_loss': 0.1851515918970108, 'eval_runtime': 14.6488, 'eval_samples_per_second': 16.725, 'eval_steps_per_second': 8.397, 'epoch': 0.99}
{'loss': 0.1918, 'learning_rate': 4.0340909090909095e-06, 'epoch': 1.02}
{'loss': 0.1755, 'learning_rate': 3.9772727272727275e-06, 'epoch': 1.04}
{'loss': 0.1825, 'learning_rate': 3.9204545454545456e-06, 'epoch': 1.07}
{'loss': 0.1721, 'learning_rate': 3.863636363636364e-06, 'epoch': 1.1}
{'loss': 0.1723, 'learning_rate': 3.806818181818182e-06, 'epoch': 1.13}
{'loss': 0.1774, 'learning_rate': 3.7500000000000005e-06, 'epoch': 1.15}
{'loss': 0.1639, 'learning_rate': 3.6931818181818186e-06, 'epoch': 1.18}
{'loss': 0.1877, 'learning_rate': 3.6363636363636366e-06, 'epoch': 1.21}
{'loss': 0.1811, 'learning_rate': 3.579545454545455e-06, 'epoch': 1.24}
{'loss': 0.1814, 'learning_rate': 3.522727272727273e-06, 'epoch': 1.26}
{'loss': 0.1701, 'learning_rate': 3.4659090909090915e-06, 'epoch': 1.29}
{'loss': 0.1877, 'learning_rate': 3.409090909090909e-06, 'epoch': 1.32}
{'loss': 0.1555, 'learning_rate': 3.352272727272727e-06, 'epoch': 1.35}
{'loss': 0.146, 'learning_rate': 3.2954545454545456e-06, 'epoch': 1.37}
{'loss': 0.1949, 'learning_rate': 3.2386363636363637e-06, 'epoch': 1.4}
{'loss': 0.1548, 'learning_rate': 3.181818181818182e-06, 'epoch': 1.43}
{'loss': 0.1657, 'learning_rate': 3.125e-06, 'epoch': 1.46}
{'loss': 0.186, 'learning_rate': 3.0681818181818186e-06, 'epoch': 1.48}
{'loss': 0.1615, 'learning_rate': 3.0113636363636366e-06, 'epoch': 1.51}
{'loss': 0.1918, 'learning_rate': 2.954545454545455e-06, 'epoch': 1.54}
{'loss': 0.1584, 'learning_rate': 2.897727272727273e-06, 'epoch': 1.57}
{'loss': 0.1525, 'learning_rate': 2.8409090909090916e-06, 'epoch': 1.59}
{'loss': 0.1668, 'learning_rate': 2.784090909090909e-06, 'epoch': 1.62}
{'loss': 0.1579, 'learning_rate': 2.7272727272727272e-06, 'epoch': 1.65}
{'loss': 0.151, 'learning_rate': 2.6704545454545457e-06, 'epoch': 1.68}
{'loss': 0.1721, 'learning_rate': 2.6136363636363637e-06, 'epoch': 1.7}
{'loss': 0.1775, 'learning_rate': 2.556818181818182e-06, 'epoch': 1.73}
{'loss': 0.1747, 'learning_rate': 2.5e-06, 'epoch': 1.76}
{'loss': 0.1486, 'learning_rate': 2.4431818181818182e-06, 'epoch': 1.79}
{'loss': 0.1707, 'learning_rate': 2.3863636363636367e-06, 'epoch': 1.81}
{'loss': 0.1788, 'learning_rate': 2.3295454545454547e-06, 'epoch': 1.84}
{'loss': 0.1779, 'learning_rate': 2.2727272727272728e-06, 'epoch': 1.87}
{'loss': 0.1648, 'learning_rate': 2.2159090909090912e-06, 'epoch': 1.9}
{'loss': 0.1546, 'learning_rate': 2.1590909090909092e-06, 'epoch': 1.92}
{'loss': 0.1741, 'learning_rate': 2.1022727272727277e-06, 'epoch': 1.95}
{'loss': 0.1754, 'learning_rate': 2.0454545454545457e-06, 'epoch': 1.98}
{'eval_loss': 0.17209531366825104, 'eval_runtime': 14.6633, 'eval_samples_per_second': 16.708, 'eval_steps_per_second': 8.388, 'epoch': 1.98}
{'loss': 0.1657, 'learning_rate': 1.9886363636363638e-06, 'epoch': 2.01}
{'loss': 0.1654, 'learning_rate': 1.931818181818182e-06, 'epoch': 2.03}
{'loss': 0.1603, 'learning_rate': 1.8750000000000003e-06, 'epoch': 2.06}
{'loss': 0.1466, 'learning_rate': 1.8181818181818183e-06, 'epoch': 2.09}
{'loss': 0.1613, 'learning_rate': 1.7613636363636365e-06, 'epoch': 2.12}
{'loss': 0.1623, 'learning_rate': 1.7045454545454546e-06, 'epoch': 2.14}
{'loss': 0.1622, 'learning_rate': 1.6477272727272728e-06, 'epoch': 2.17}
{'loss': 0.1564, 'learning_rate': 1.590909090909091e-06, 'epoch': 2.2}
{'loss': 0.1602, 'learning_rate': 1.5340909090909093e-06, 'epoch': 2.23}
{'loss': 0.1395, 'learning_rate': 1.4772727272727275e-06, 'epoch': 2.25}
{'loss': 0.1225, 'learning_rate': 1.4204545454545458e-06, 'epoch': 2.28}
{'loss': 0.1668, 'learning_rate': 1.3636363636363636e-06, 'epoch': 2.31}
{'loss': 0.1693, 'learning_rate': 1.3068181818181819e-06, 'epoch': 2.34}
{'loss': 0.1696, 'learning_rate': 1.25e-06, 'epoch': 2.36}
{'loss': 0.142, 'learning_rate': 1.1931818181818183e-06, 'epoch': 2.39}
{'loss': 0.148, 'learning_rate': 1.1363636363636364e-06, 'epoch': 2.42}
{'loss': 0.1925, 'learning_rate': 1.0795454545454546e-06, 'epoch': 2.45}
{'loss': 0.1887, 'learning_rate': 1.0227272727272729e-06, 'epoch': 2.47}
{'loss': 0.1475, 'learning_rate': 9.65909090909091e-07, 'epoch': 2.5}
{'loss': 0.1542, 'learning_rate': 9.090909090909091e-07, 'epoch': 2.53}
{'loss': 0.162, 'learning_rate': 8.522727272727273e-07, 'epoch': 2.56}
{'loss': 0.1677, 'learning_rate': 7.954545454545455e-07, 'epoch': 2.58}
{'loss': 0.1548, 'learning_rate': 7.386363636363638e-07, 'epoch': 2.61}
{'loss': 0.1496, 'learning_rate': 6.818181818181818e-07, 'epoch': 2.64}
{'loss': 0.1462, 'learning_rate': 6.25e-07, 'epoch': 2.67}
{'loss': 0.1718, 'learning_rate': 5.681818181818182e-07, 'epoch': 2.69}
{'loss': 0.1639, 'learning_rate': 5.113636363636364e-07, 'epoch': 2.72}
{'loss': 0.1464, 'learning_rate': 4.5454545454545457e-07, 'epoch': 2.75}
{'loss': 0.1636, 'learning_rate': 3.9772727272727276e-07, 'epoch': 2.78}
{'loss': 0.1397, 'learning_rate': 3.409090909090909e-07, 'epoch': 2.8}
{'loss': 0.1416, 'learning_rate': 2.840909090909091e-07, 'epoch': 2.83}
{'loss': 0.1518, 'learning_rate': 2.2727272727272729e-07, 'epoch': 2.86}
{'loss': 0.1844, 'learning_rate': 1.7045454545454545e-07, 'epoch': 2.89}
{'loss': 0.1669, 'learning_rate': 1.1363636363636364e-07, 'epoch': 2.91}
{'loss': 0.1439, 'learning_rate': 5.681818181818182e-08, 'epoch': 2.94}
{'loss': 0.1515, 'learning_rate': 0.0, 'epoch': 2.97}
{'eval_loss': 0.17120030522346497, 'eval_runtime': 14.6524, 'eval_samples_per_second': 16.721, 'eval_steps_per_second': 8.395, 'epoch': 2.97}
{'train_runtime': 2683.1297, 'train_samples_per_second': 5.205, 'train_steps_per_second': 0.04, 'train_loss': 0.20910147425753098, 'epoch': 2.97}
NAME
    train.py

SYNOPSIS
    train.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     json
       JSON (JavaScript Object Notation) <http://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     VLLMInferenceModel
       Simple vLLM Inference Wrapper for text generation and logprobs.

     get_formatted_responses
       Formats prompts and returns formatted model responses.

     mutual_information
       Computes mutual information.

     DataCollatorForSupervisedDataset
       Data collator for SFT which masks user from the loss.

     preprocess
       Preprocess the data by tokenizing.

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX
