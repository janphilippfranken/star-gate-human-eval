[2024-07-03 13:06:10,849][root][INFO] - Writing to: /scr/jphilipp/stargate/checkpoints/m1-cot-eig
learning rate: 1e-05
[2024-07-03 13:06:19,720][root][INFO] - DATA
[{'role': 'user', 'content': 'How do I make a Mediterranean salad.'}, {'role': 'assistant', 'content': 'What type of occasion or event are you planning to serve this Mediterranean salad for, or are you looking to prepare it for a quick weeknight dinner or lunch?'}, {'role': 'user', 'content': 'Foraged food, no occasion, no event, self-sufficient.'}, {'role': 'assistant', 'content': "You can create a unique and delicious Mediterranean salad by foraging for wild greens like dandelion, chicory, or wild arugula, and combining them with foraged herbs like thyme, rosemary, or oregano. Add some wild garlic or ramps for a pungent flavor, and top it off with a drizzle of olive oil and a squeeze of fresh lemon juice. Don't forget to include some foraged nuts or seeds like pine nuts or sunflower seeds for added crunch."}]
DatasetDict({
    train: Dataset({
        features: ['input_ids', 'labels', 'attention_mask'],
        num_rows: 4512
    })
    test: Dataset({
        features: ['input_ids', 'labels', 'attention_mask'],
        num_rows: 238
    })
})
[2024-07-03 13:06:26,173][accelerate.utils.other][WARNING] - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
{'loss': 0.3632, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.03}
{'loss': 0.4144, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.06}
{'loss': 0.4894, 'learning_rate': 1.5e-06, 'epoch': 0.09}
{'loss': 0.414, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.11}
{'loss': 0.4303, 'learning_rate': 2.5e-06, 'epoch': 0.14}
{'loss': 0.3522, 'learning_rate': 3e-06, 'epoch': 0.17}
{'loss': 0.3231, 'learning_rate': 3.5e-06, 'epoch': 0.2}
{'loss': 0.3931, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.23}
{'loss': 0.2889, 'learning_rate': 4.5e-06, 'epoch': 0.26}
{'loss': 0.3265, 'learning_rate': 5e-06, 'epoch': 0.28}
{'loss': 0.2829, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.31}
{'loss': 0.301, 'learning_rate': 6e-06, 'epoch': 0.34}
{'loss': 0.2425, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.37}
{'loss': 0.2256, 'learning_rate': 7e-06, 'epoch': 0.4}
{'loss': 0.2395, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.43}
{'loss': 0.238, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.45}
{'loss': 0.1975, 'learning_rate': 8.5e-06, 'epoch': 0.48}
{'loss': 0.2041, 'learning_rate': 9e-06, 'epoch': 0.51}
{'loss': 0.2341, 'learning_rate': 9.5e-06, 'epoch': 0.54}
{'loss': 0.1861, 'learning_rate': 1e-05, 'epoch': 0.57}
{'loss': 0.2081, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.6}
{'loss': 0.206, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.62}
{'loss': 0.1997, 'learning_rate': 9.4e-06, 'epoch': 0.65}
{'loss': 0.202, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.68}
{'loss': 0.2214, 'learning_rate': 9e-06, 'epoch': 0.71}
{'loss': 0.2036, 'learning_rate': 8.8e-06, 'epoch': 0.74}
{'loss': 0.2106, 'learning_rate': 8.6e-06, 'epoch': 0.77}
{'loss': 0.223, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.79}
{'loss': 0.2098, 'learning_rate': 8.2e-06, 'epoch': 0.82}
{'loss': 0.1803, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.85}
{'loss': 0.2441, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.88}
{'loss': 0.1799, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.91}
{'loss': 0.1821, 'learning_rate': 7.4e-06, 'epoch': 0.94}
{'loss': 0.1977, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.96}
{'loss': 0.1851, 'learning_rate': 7e-06, 'epoch': 0.99}
{'eval_loss': 0.191373810172081, 'eval_runtime': 13.59, 'eval_samples_per_second': 17.513, 'eval_steps_per_second': 8.756, 'epoch': 0.99}
{'loss': 0.1587, 'learning_rate': 6.800000000000001e-06, 'epoch': 1.02}
{'loss': 0.1477, 'learning_rate': 6.600000000000001e-06, 'epoch': 1.05}
{'loss': 0.1506, 'learning_rate': 6.4000000000000006e-06, 'epoch': 1.08}
{'loss': 0.1243, 'learning_rate': 6.200000000000001e-06, 'epoch': 1.11}
{'loss': 0.1567, 'learning_rate': 6e-06, 'epoch': 1.13}
{'loss': 0.152, 'learning_rate': 5.8e-06, 'epoch': 1.16}
{'loss': 0.1499, 'learning_rate': 5.600000000000001e-06, 'epoch': 1.19}
{'loss': 0.1602, 'learning_rate': 5.400000000000001e-06, 'epoch': 1.22}
{'loss': 0.1365, 'learning_rate': 5.2e-06, 'epoch': 1.25}
{'loss': 0.1582, 'learning_rate': 5e-06, 'epoch': 1.28}
{'loss': 0.126, 'learning_rate': 4.800000000000001e-06, 'epoch': 1.3}
{'loss': 0.1541, 'learning_rate': 4.600000000000001e-06, 'epoch': 1.33}
{'loss': 0.1416, 'learning_rate': 4.4e-06, 'epoch': 1.36}
{'loss': 0.1717, 'learning_rate': 4.2000000000000004e-06, 'epoch': 1.39}
{'loss': 0.1407, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.42}
{'loss': 0.1396, 'learning_rate': 3.8000000000000005e-06, 'epoch': 1.45}
{'loss': 0.1469, 'learning_rate': 3.6000000000000003e-06, 'epoch': 1.48}
{'loss': 0.1558, 'learning_rate': 3.4000000000000005e-06, 'epoch': 1.5}
{'loss': 0.1276, 'learning_rate': 3.2000000000000003e-06, 'epoch': 1.53}
{'loss': 0.1138, 'learning_rate': 3e-06, 'epoch': 1.56}
{'loss': 0.1149, 'learning_rate': 2.8000000000000003e-06, 'epoch': 1.59}
{'loss': 0.1382, 'learning_rate': 2.6e-06, 'epoch': 1.62}
{'loss': 0.1321, 'learning_rate': 2.4000000000000003e-06, 'epoch': 1.65}
{'loss': 0.126, 'learning_rate': 2.2e-06, 'epoch': 1.67}
{'loss': 0.1462, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.7}
{'loss': 0.1373, 'learning_rate': 1.8000000000000001e-06, 'epoch': 1.73}
{'loss': 0.1371, 'learning_rate': 1.6000000000000001e-06, 'epoch': 1.76}
{'loss': 0.1583, 'learning_rate': 1.4000000000000001e-06, 'epoch': 1.79}
{'loss': 0.1451, 'learning_rate': 1.2000000000000002e-06, 'epoch': 1.82}
{'loss': 0.1519, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.84}
{'loss': 0.14, 'learning_rate': 8.000000000000001e-07, 'epoch': 1.87}
{'loss': 0.1152, 'learning_rate': 6.000000000000001e-07, 'epoch': 1.9}
{'loss': 0.1201, 'learning_rate': 4.0000000000000003e-07, 'epoch': 1.93}
{'loss': 0.1531, 'learning_rate': 2.0000000000000002e-07, 'epoch': 1.96}
{'loss': 0.1281, 'learning_rate': 0.0, 'epoch': 1.99}
{'eval_loss': 0.1843215823173523, 'eval_runtime': 13.6074, 'eval_samples_per_second': 17.49, 'eval_steps_per_second': 8.745, 'epoch': 1.99}
{'train_runtime': 1725.4306, 'train_samples_per_second': 5.23, 'train_steps_per_second': 0.041, 'train_loss': 0.2022345442857061, 'epoch': 1.99}
NAME
    train.py

SYNOPSIS
    train.py GROUP | COMMAND | VALUE

GROUPS
    GROUP is one of the following:

     os
       OS routines for NT or Posix depending on what system we're on.

     json
       JSON (JavaScript Object Notation) <http://json.org> is a subset of JavaScript syntax (ECMA-262 3rd edition) used as a lightweight data interchange format.

     fire
       The Python Fire module.

     hydra

     torch
       The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.

     wandb
       Use wandb to track machine learning work.

     logging
       Logging package for Python. Based on PEP 282 and comments thereto in comp.lang.python.

     Dict
       A generic version of dict.

     Sequence
       A generic version of collections.abc.Sequence.

     List
       A generic version of list.

     copy
       Generic (shallow and deep) copying operations.

     transformers

COMMANDS
    COMMAND is one of the following:

     DictConfig
       Container tagging interface

     OmegaConf
       OmegaConf primary class

     TrainingArguments
       TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.

     Trainer
       Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.

     AutoModelForCausalLM
       This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created with the [`~AutoModelForCausalLM.from_pretrained`] class method or the [`~AutoModelForCausalLM.from_config`] class method.

     AutoTokenizer
       This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when created with the [`AutoTokenizer.from_pretrained`] class method.

     dataclass
       Returns the same class as was passed in, with dunder methods added based on the fields defined in the class.

     Dataset
       A Dataset backed by an Arrow table.

     VLLMInferenceModel
       Simple vLLM Inference Wrapper for text generation and logprobs.

     get_formatted_responses
       Formats prompts and returns formatted model responses.

     mutual_information
       Computes mutual information.

     DataCollatorForSupervisedDataset
       Data collator for SFT which masks user from the loss.

     preprocess
       Preprocess the data by tokenizing.

     main

VALUES
    VALUE is one of the following:

     IGNORE_INDEX
