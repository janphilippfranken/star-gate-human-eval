/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s]Processed prompts:  20%|██        | 1/5 [00:02<00:10,  2.62s/it]Processed prompts:  60%|██████    | 3/5 [00:02<00:01,  1.37it/s]Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  2.39it/s]Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.67it/s]
Processed prompts:   0%|          | 0/175 [00:00<?, ?it/s]Processed prompts:   1%|          | 1/175 [00:40<1:56:32, 40.19s/it]Processed prompts:   1%|          | 2/175 [00:40<48:03, 16.67s/it]  Processed prompts:   2%|▏         | 3/175 [00:40<26:29,  9.24s/it]Processed prompts:   2%|▏         | 4/175 [00:41<16:15,  5.71s/it]Processed prompts:   3%|▎         | 5/175 [00:41<10:37,  3.75s/it]Processed prompts:   3%|▎         | 6/175 [00:41<07:14,  2.57s/it]Processed prompts:   4%|▍         | 7/175 [00:42<05:11,  1.85s/it]Processed prompts:   5%|▌         | 9/175 [00:42<03:02,  1.10s/it]Processed prompts:   6%|▌         | 10/175 [00:43<02:39,  1.03it/s]Processed prompts:   6%|▋         | 11/175 [00:43<02:08,  1.27it/s]Processed prompts:   7%|▋         | 12/175 [00:43<01:45,  1.55it/s]Processed prompts:   9%|▊         | 15/175 [00:44<01:01,  2.62it/s]Processed prompts:  12%|█▏        | 21/175 [00:44<00:27,  5.53it/s]Processed prompts:  15%|█▍        | 26/175 [00:44<00:19,  7.75it/s]Processed prompts:  21%|██        | 37/175 [00:45<00:08, 15.37it/s]Processed prompts:  23%|██▎       | 40/175 [00:45<00:08, 15.66it/s]Processed prompts:  27%|██▋       | 47/175 [00:45<00:06, 18.70it/s]Processed prompts:  33%|███▎      | 57/175 [00:45<00:04, 27.01it/s]Processed prompts:  39%|███▉      | 69/175 [00:45<00:02, 38.12it/s]Processed prompts:  43%|████▎     | 75/175 [00:45<00:02, 39.44it/s]Processed prompts:  46%|████▌     | 80/175 [00:46<00:02, 39.31it/s]Processed prompts:  52%|█████▏    | 91/175 [00:46<00:01, 51.19it/s]Processed prompts:  56%|█████▌    | 98/175 [00:46<00:01, 53.86it/s]Processed prompts:  60%|██████    | 105/175 [00:46<00:01, 56.88it/s]Processed prompts:  65%|██████▌   | 114/175 [00:46<00:01, 58.71it/s]Processed prompts:  69%|██████▉   | 121/175 [00:46<00:00, 57.21it/s]Processed prompts:  76%|███████▌  | 133/175 [00:46<00:00, 69.66it/s]Processed prompts:  81%|████████  | 141/175 [00:46<00:00, 67.87it/s]Processed prompts:  85%|████████▌ | 149/175 [00:47<00:00, 56.67it/s]Processed prompts:  90%|█████████ | 158/175 [00:47<00:00, 58.36it/s]Processed prompts:  94%|█████████▍| 165/175 [00:47<00:00, 45.57it/s]Processed prompts:  98%|█████████▊| 171/175 [00:48<00:00, 24.25it/s]Processed prompts: 100%|██████████| 175/175 [00:48<00:00, 20.20it/s]Processed prompts: 100%|██████████| 175/175 [00:48<00:00,  3.62it/s]
ERROR: Cannot find key: seed=0
Usage: conversations.py <group|command|value>
  available groups:      json | fire | torch | hydra | logging
  available commands:    DictConfig | AutoTokenizer | VLLMInferenceModel | main
  available values:      ORACLE_PROMPT | RESPONSE_PROMPT | PROMPT_LOGPROBS |
                         QUESTION_PROMPT | ROLEPLAY_PROMPT

For detailed information on this command, run:
  conversations.py --help
